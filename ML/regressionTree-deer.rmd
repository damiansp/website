Regression Trees to Predict Fetal Health in White-Tailed Deer
=============================================================

In the statistics section, I presented an analysis of the robustness of fetal deer, where the "robustness" was defined as the ratio: ln(mass) / ln(length).  In general, the higher this ratio, the healthier the fetus, and this in turn is a good predictor that the fawn will survive its first year after birth.

In that section, a generalized linear model was used, and indicated that the sex and number of fetuses, age of the mother, and intensity of culling (localized killing of deer for disease control) were all significant, as were a number of interaction and nonlinear effects.  However, the fit of that model was not especially good (Nagelkerke's r^2 = 0.19), so here I investigate how a regression tree performs on the same analysis, with the same set of available predictors.  After finding a decent regression tree, I use k-folds to determine how well each method performs at predicting novel data.

```{r setup}
rm(list = ls())
library(rpart)

deer <- read.csv('~/Desktop/NewDeer/Data/Fetus_Final.csv')
deer <- subset(deer, days > 31) # see the <a href="http://phillips-research.com/deerSS2.html">"Fetus Detection"</a> section.
deer$age <- as.factor(deer$age3categories)
# reorder levels to be increasing ages
deer$age <- factor(deer$age, levels = c('fawn', 'yearling', 'adult'))
names(deer)[46] <- 'intensity'
names(deer)[50] <- 'cwd' # disease status for chronic wasting disease

# I treat year as a factor because I am not looking for trends over time, but do 
# want to allow for annual variation that might be due to precipitation, temperature,
# etc., that may either affect the mothers directly, or indirectly by affecting their
# food resources.
deer$yearF <- as.factor(deer$year)

# calculate the fetal size ("robustness") statistic.  (For each pregnant mother, I
# take the average of all her fetuses).
deer$fsRatio <- log(deer$wtMean) / log(deer$crMean)

# Because of the log/log ratio we get some -Inf values, as well as NA values where 
# one or more of the inputs was missing. Remove these records:
problems = which(!is.finite(deer$fsRatio))

# verify
# deer$fsRatio[problems]
deer = deer[-problems, ]

# Initial regression tree for fetal size ratio
fsr.rt <- rpart(fsRatio ~ maleNo + fetusNo + age + intensity + yearF + cwd, 
				data = deer)
fsr.rt

# show as tree
plot(fsr.rt)
text(fsr.rt)
```

This tree is our initial go, and can be read as follows.  If the mother is either a yearling or adult, the expected fetal size ratio is 1.859 (right branch).  If the mother is a fawn, the expected ratio is 1.664.

This same result can be arrived at by simply taking the means of each group:
```{r age_means}
(group.means <- tapply(deer$fsRatio[is.finite(deer$fsRatio)], 
	   				   (deer$age == 'fawn')[is.finite(deer$fsRatio)], mean))
```

This is the default tree which the rpart library returns to us in an effort to fit the best tree without overfitting.

We can visualize the affect of the tree split by plotting the data.
```{r visualize}
plot(deer$fsRatio ~ jitter(as.numeric(deer$age)), pch = 16, 
	 col = rgb(0, 0, 0, 0.2), xlab = 'Age', ylab = 'Fetal size ratio', 
	 xaxt = 'n')
axis(1, labels = c('fawn', 'yearling', 'adult'), at = c(1:3))
abline(h = mean(deer$fsRatio), col = 'grey')
abline(v = 1.5, lwd = 2)
lines(c(0, 1.5), rep(group.means[2], 2), col = 2)
lines(c(1.5, 3.5), rep(group.means[1], 2), col = 4)
legend('bottomright', lwd = c(2, 1, 1, 1), col = c('black', 'grey', 'red', 'blue'),
	   legend = c('partition', 'overall mean', 'fawn mean', 
	   			  'yearling/adult mean'))
```

One problem that immediately stands out is that a mean is being used to predict central tendencies, even though the data (especially for fawns) are skewed.  Attempt to normalize the data.

```{r normalize}
hist(deer$fsRatio, main = '', xlab = 'Fetal size ratio')

# shift values to all be non-negative
fs.trans = deer$fsRatio - min(deer$fsRatio)
hist(fs.trans)

# try exponential transforms to minimize the p-value of the Shapiro-Wilk test for 
# normality

(best.p <- shapiro.test(fs.trans)$p)  
# untransformed is highly non-normal, as expected
exponents = seq(6, 10, 0.1)
ps = numeric(length(exponents))

for (e in 1:length(exponents)) {
	ps[e] = shapiro.test(fs.trans^exponents[e])$p
}

plot(ps ~ exponents, type = 'l')


# The optimal value is somewhere around 9-- make finer resolution tests around 
# this value

exponents = seq(8.8, 9.5, 0.01)
ps <- numeric(length(exponents))

for (e in 1:length(exponents)) {
	ps[e] <- shapiro.test(fs.trans^exponents[e])$p
}

plot(ps ~ exponents, type = 'l')
best.p.index <- which(ps == max(ps))
abline(h = ps[best.p.index], col = 'grey')
abline(v = exponents[best.p.index], lty = 2)

fs.trans <- fs.trans^9.15
hist(fs.trans, main = '', xlab = expression(('fetal size ratio')^9.15))
```

Now reattempt the tree.
```{r tree_transformed}
fsr.rt2 <- rpart(fs.trans ~ maleNo + fetusNo + age + intensity + yearF + cwd, 
				data = deer)
fsr.rt2

# show as tree
plot(fsr.rt2, compress = T, margin = 0.1)
text(fsr.rt2)

# Also, create a function to revert transformed values back to original scale:
fs.inverse = function(x) {
	x^(1/9.5) + min(deer$fsRatio)
}

# Convert end node values
ends <- c(7554.199, 10087.160, 11509.130)
fs.inverse(ends)
```

Fawns are predicted to have fetuses with an fs ratio of 1.68.  For adults and yearlings, the predicted ratio is 1.76 in years 2008, '09, '10, and '12, and 1.80 in 2003 - 07 and 2011.  In the generalized linear model, a number of squared terms were also significant, so let's consider those here.

```{r squared_terms}
fsr.rt3 <- rpart(fs.trans ~ maleNo + I(maleNo^2) + fetusNo + I(fetusNo^2) + age + 
				 intensity + I(intensity^2) + yearF + cwd, data = deer) 
fsr.rt3
```

This apppears to have had no effect.

Before moving on, let's again visualize the effect of the existing partitions.
First let's color-code fetal size ratios so that they transition from green at the minimum value to red at the max, so that color can serve as a visual proxy for this value.

```{r color_vis}
colorScale = colorRamp(c('green', 'red'))(fs.trans / max(fs.trans))
deer$fs.colors <- rgb(colorScale[, 1] / 255, colorScale[, 2] / 255, 0)

# also group years by tree decision
deer$yrA <- deer$yearF %in% c('2008','2009', '2010', '2012')

plot(jitter(as.numeric(age), factor = 2) ~ jitter(as.numeric(yrA), factor = 2), 
	 col = fs.colors, data = deer, pch = 16, xlab = 'Year Group', 
	 ylab = 'Age Group', xaxt = 'n', yaxt = 'n')
axis(1, at = c(0, 1), labels = c('2003 - 07, 2011', '2008 - 10, 2012'))
axis(2, at = c(1, 2, 3), labels = c('fawn', 'yearling', 'adult'))
abline(h = 1.5, lwd = 2)
text(0, 1.55, 'First Partition')
lines(c(0.5, 0.5), c(1.5, 3.5), lwd = 2)
text(0.45, 2.5, 'Second Partition', srt = 90)
```

Now examine the cross-validation error at different levels of complexity in the tree.  We choose the cp (complexity parameter) that minimizes the cross-validation error.

```{r choose_cp}
set.seed(42)
fsr.rt4 <- rpart(fs.trans ~ maleNo + I(maleNo^2) + fetusNo + I(fetusNo^2) + age + 
				 intensity + I(intensity^2) + yearF + cwd, data = deer, 
				 cp = 0.001) 
plotcp(fsr.rt4)

fsr.rt4.prune = prune.rpart(fsr.rt4, 0.0039245)

plot(fsr.rt4.prune, uniform = T, compress = T)
text(fsr.rt4.prune, cex = 0.6)
```

As this is still ultimately a regression model, we can perform the same diagnostics, and use the sum of squared errors (SSE) of the model residuals relative to the squared deviations of the fetal size ratios from the group mean as a metric of goodness of fit.
```{r diagnostics}
plot(jitter(predict(fsr.rt4.prune), factor = 5), resid(fsr.rt4.prune), 
	 xlab = 'Fitted', ylab = 'Residuals')
qqnorm(resid(fsr.rt4.prune))
qqline(resid(fsr.rt4.prune))

# Goodness of fit
1 - sum(resid(fsr.rt4.prune)^2) / sum((fs.trans - mean(fs.trans))^2)
```

Compared to the GLM, the fit would appear to be poorer.  But to actually compare each method directly, let's do a quick and dirty comparison between the regression tree and a similar linear model.  Rather than doing k-folds, which is ideal, for brevity I will just do a single test.

First split the data into test and development sets:

```{r dev_test}
dim(deer) 
# 2422 rows, so put aside 250 (randomly chosen) for testing.
test.records = sample(1:2422, size = 250)
deer$fs.trans = fs.trans
rm(fs.trans)

test.set = deer[test.records, ]
dev.set = deer[-test.records, ]

# First the regression tree
fsr.rt = rpart(fs.trans ~ maleNo + I(maleNo^2) + fetusNo + I(fetusNo^2) + age + 
			   intensity + I(intensity^2) + yearF + cwd, data = dev.set, 
			   cp = 0.001) 
plotcp(fsr.rt)
# In actuality, the cross-validation run here is also randomized, so the cp value
# with the lowest cross-validation error will vary between runs.  The researcher 
# could repeat the test many times and keep track of the best cp value, or some 
# similar approach.  Here though, I simply take the best value from the first run.

printcp(fsr.rt)

fsr.rt.prune = prune.rpart(fsr.rt, cp = 0.0044364)
fsr.rt.prune
plot(fsr.rt.prune, uniform = T, compress = T)
text(fsr.rt.prune, cex = 0.6)
```

It is reassuring that the splits are almost (though not perfectly) identical to those we got above when using the full data set.  As before, check diagnostics and goodness-of-fit, then go on to make predictions with the test set.
```{r diagnostics2}
plot(jitter(predict(fsr.rt.prune), factor = 5), resid(fsr.rt.prune), 
	 xlab = 'Fitted', ylab = 'Residuals')
qqnorm(resid(fsr.rt.prune))
qqline(resid(fsr.rt.prune))
1 - sum(resid(fsr.rt.prune)^2) / 
  sum((dev.set$fs.trans - mean(dev.set$fs.trans))^2)

# Make predictions and save SSE
rt.preds = predict(fsr.rt.prune, newdata = test.set)
(rs.sse = sum((rt.preds - test.set$fs.trans)^2))
```

Now do the same with a LM.
```{r glm}
#fsr.lm = lm(fs.trans ~ maleNo + I(maleNo^2) + fetusNo + I(fetusNo^2) + age + 
#			intensity + I(intensity^2) + yearF + cwd, data = dev.set)

# fsr.lm = step(fsr.lm, direction = 'both')
fsr.lm = lm(fs.trans ~ fetusNo + I(fetusNo^2) + age + intensity + I(intensity^2) +
			yearF + cwd, data = dev.set)
summary(fsr.lm)
# The goodness of fit is quite similar to the regression tree.
# Diagnostics:
par(mfrow = c(2, 2))
plot(fsr.lm)
# ...also look pretty good... maybe a very slight increase in variance of the 
# residuals as fitted values increase...
# And now the moment of truth:
lm.preds = predict(fsr.lm, newdata = test.set)
(lm.sse = sum((lm.preds[!is.na(lm.preds)] - 
  test.set$fs.trans[!is.na(lm.preds)])^2))

# Because we got an NA in the lm predictions, we have to compare SSE per number of 
# predictions:
cbind(rs.sse / 250, lm.sse / 249)
```

The error is less for the linear model in this case, and the astute observer may have noticed too, that the model employed here was somewhat simpler than the one given in statistics section (which was even better fitting) for the sake of making the two models directly comparable.  Nevertheless, regression trees are very fast, and in some cases a fast approximation may be preferred to a more precise, but also more time-consuming analysis.  And, of course, there is no guarantee that a linear model will always outperform an equivalent regression tree.  Note too, that the regression tree is somewhat more succinct in that, with 10 splits, it effectively has 11 parameters, whereas the linear model has 17.